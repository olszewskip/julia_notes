{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: CUDAdrv.jl failed to initialize, GPU functionality unavailable (set JULIA_CUDA_SILENT or JULIA_CUDA_VERBOSE to silence or expand this message)\n",
      "└ @ CUDAdrv /home/olszewskip/.julia/packages/CUDAdrv/mCr0O/src/CUDAdrv.jl:69\n"
     ]
    }
   ],
   "source": [
    "using Zygote\n",
    "using Flux: σ, softmax, logitcrossentropy, Chain, Optimise, onehotbatch, onecold, Dense\n",
    "using Flux.Data: MNIST\n",
    "using SparseArrays\n",
    "using StatsBase: sample, shuffle, mean\n",
    "import Base: broadcast, broadcasted\n",
    "#using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MNIST.labels();\n",
    "label_pool = sort(Vector(collect(Set(labels))));\n",
    "y = onehotbatch(labels, label_pool);\n",
    "\n",
    "imgs = MNIST.images();\n",
    "# lighter_than(pixel, threshold) = pixel.val > threshold\n",
    "# function lightness_threshold(pixel_array::Array{T, 2}, threshold) where T\n",
    "#     convert.(T, lighter_than.(pixel_array, threshold))\n",
    "# end\n",
    "# imgs_blackenwhite = lightness_threshold.(imgs, 0.7);\n",
    "#X = hcat(convert.(Array{Float32, 1}, reshape.(imgs_blackenwhite, :))...);\n",
    "X = hcat(convert.(Array{Float32, 1}, reshape.(imgs, :))...);\n",
    "\n",
    "index_train = floor(Int, 0.7 * size(X, 2))\n",
    "index_val = index_train + floor(Int, 0.2 * size(X, 2))\n",
    "X_train = X[:, 1:index_train];\n",
    "y_train = y[:, 1:index_train];\n",
    "X_val = X[:, index_train+1:index_val];\n",
    "y_val = y[:, index_train+1:index_val];\n",
    "X_test = X[:, index_val+1:end];\n",
    "y_test = y[:, index_val+1:end];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct SprAffine{S,T,F}\n",
    "    W::S\n",
    "    b::T\n",
    "    σ::F\n",
    "end\n",
    "\n",
    "function Base.show(io::IO, layer::SprAffine)\n",
    "  print(io, \"SprAffine(\", size(layer.W, 2), \", \", size(layer.W, 1))\n",
    "  print(io, \", \", round(length(layer.W.nzval) / length(layer.W), sigdigits=6))\n",
    "  layer.σ == identity || print(io, \", \", layer.σ)\n",
    "  print(io, \")\")\n",
    "end\n",
    "\n",
    "SprAffine(in::Number, out::Number, frac::AbstractFloat, σ::Function=identity) =\n",
    "    #SprAffine(spones(out, in, frac), spzeros(out), fraq, σ)\n",
    "    SprAffine(sprandn(Float32, out, in, frac), zeros(Float32, out), σ)\n",
    "\n",
    "(layer::SprAffine)(x) = layer.σ.(layer.W * x .+ layer.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(model, X, y)\n",
    "    return logitcrossentropy(model(X), y)\n",
    "end\n",
    "accuracy(model, X, y) = mean(onecold(softmax(model(X))) .== onecold(y));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_model (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_model(in, hidden, out, fraction)\n",
    "    return Chain(\n",
    "            SprAffine(in, hidden, fraction),\n",
    "            SprAffine(hidden, out, fraction),\n",
    "           );\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09375"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(size(X, 1), 100, size(y, 1), 0.1)\n",
    "loss(model, sparse(X[:, 1:32]), y[:, 1:32])\n",
    "accuracy(model, sparse(X[:, 1:32]), y[:, 1:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sp = sparse(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_local, back = pullback(model) do model\n",
    "    loss(model,\n",
    "         X_sp[:, 1:32],\n",
    "         y[:, 1:32])\n",
    "end;\n",
    "grads = back(1)[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flux.Optimise.Descent(0.1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Optimise.Descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set(Tuple{Int64,Int64}[(1, 151), (50, 506), (27, 552), (75, 782), (30, 197), (3, 694), (39, 486), (13, 569), (78, 513), (51, 182)  …  (52, 195), (70, 266), (58, 37), (89, 292), (88, 487), (22, 81), (83, 162), (39, 244), (76, 131), (35, 224)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I,J,_ = findnz(model[1].W)\n",
    "weights_elems = Set(zip(I, J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set(Tuple{Int64,Int64}[(69, 598), (29, 164), (96, 716), (95, 522), (47, 153), (70, 631), (59, 479), (69, 239), (85, 547), (96, 544)  …  (80, 458), (97, 320), (86, 595), (73, 552), (59, 490), (2, 262), (22, 681), (83, 162), (21, 220), (39, 447)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I,J,_ = findnz(getfield(grads.layers[1], 1).W)\n",
    "grad_elems = Set(zip(I, J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 444)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(intersect(weights_elems, grad_elems))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8742305f0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[1].W[11, 444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.011893435f0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getfield(grads.layers[1], 1).W[11, 444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_!(opt, model[1].W, getfield(grads.layers[1], 1).W);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "broadcasted (generic function with 94 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function broadcast(-, arr_sink::AbstractSparseMatrix{Tv,Ti},\n",
    "                      arr_source::AbstractSparseMatrix{Tv,Ti}) where {Tv,Ti}\n",
    "    for (col_index, (ptr_sink, end_sink)) in enumerate(zip(arr_sink.colptr[1:end-1], arr_sink.colptr[2:end]))\n",
    "        ptr_source = arr_source.colptr[col_index]\n",
    "        end_source = arr_source.colptr[col_index + 1]\n",
    "        while ptr_sink < end_sink && ptr_source < end_source\n",
    "            #println(ptr_sink, \" \", arr_sink.rowval[ptr_sink], \" \", ptr_source, \" \", arr_source.rowval[ptr_source])\n",
    "            if arr_sink.rowval[ptr_sink] < arr_source.rowval[ptr_source]\n",
    "                #println(\"sink up\")\n",
    "                ptr_sink += 1\n",
    "                continue\n",
    "            end\n",
    "            if arr_sink.rowval[ptr_sink] > arr_source.rowval[ptr_source]\n",
    "                #println(\"source up\")\n",
    "                ptr_source += 1\n",
    "                continue\n",
    "            end\n",
    "            row_index = arr_sink.rowval[ptr_sink]\n",
    "            #println(row_index, \" \", col_index, \" \", arr_sink.nzval[ptr_sink], \" \", arr_source.nzval[ptr_source])\n",
    "            arr_sink.nzval[ptr_sink] -= arr_source.nzval[ptr_source]\n",
    "            ptr_sink += 1\n",
    "            ptr_source += 1\n",
    "        end\n",
    "    end\n",
    "    return arr_sink\n",
    "end\n",
    "\n",
    "function broadcasted(-, arr_sink::AbstractSparseMatrix{Tv,Ti},\n",
    "                        arr_source::AbstractSparseMatrix{Tv,Ti}) where {Tv,Ti}\n",
    "    broadcast(-, arr_sink, arr_source)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_! (generic function with 3 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_!(opt, model, grads::Nothing) = model\n",
    "\n",
    "# function update_!(opt::Optimise.Descent, arr::AbstractSparseMatrix, d_arr::AbstractSparseMatrix)\n",
    "#     for (col_index, (ptr_sink, end_sink)) in enumerate(zip(arr.colptr[1:end-1], d_arr.colptr[2:end]))\n",
    "#         ptr_source = d_arr.colptr[col_index]\n",
    "#         end_source = d_arr.colptr[col_index + 1]\n",
    "#         while ptr_sink < end_sink && ptr_source < end_source\n",
    "#             if arr.rowval[ptr_sink] < d_arr.rowval[ptr_source]\n",
    "#                 #println(\"sink up\")\n",
    "#                 ptr_sink += 1\n",
    "#                 continue\n",
    "#             end\n",
    "#             if arr.rowval[ptr_sink] > d_arr.rowval[ptr_source]\n",
    "#                 #println(\"source up\")\n",
    "#                 ptr_source += 1\n",
    "#                 continue\n",
    "#             end\n",
    "#             arr.nzval[ptr_sink] -= d_arr.nzval[ptr_source] * opt.eta\n",
    "#             ptr_sink += 1\n",
    "#             ptr_source += 1\n",
    "#         end\n",
    "#     end\n",
    "# end\n",
    "\n",
    "function update_!(opt, arr::AbstractArray, d_arr::AbstractArray)\n",
    "    #print(\"abs_arr_update \")\n",
    "    Optimise.apply!(opt, arr, d_arr)\n",
    "    arr .-= d_arr\n",
    "    return arr\n",
    "end\n",
    "\n",
    "function update_!(opt, model::Chain, grads) \n",
    "    for (layer, d_layer) in zip(model.layers, grads.layers)\n",
    "        d_layer = getfield(d_layer, 1)\n",
    "        @assert nfields(layer) == nfields(d_layer)\n",
    "        for field_index in 1:nfields(layer)\n",
    "            field = getfield(layer, field_index)\n",
    "            d_field = getfield(d_layer, field_index)\n",
    "            update_!(opt, field, d_field)\n",
    "        end\n",
    "    end\n",
    "#     #print(\"down \")\n",
    "#     @assert nfields(model) == nfields(grads)\n",
    "#             [\"nfields(model) $(nfields(model)) ≠ nfields(grads) $(nfields(grads))\"]\n",
    "#     for field_idx in 1:nfields(model)\n",
    "#         field = getfield(model, field_idx)\n",
    "#         d_field = getfield(grads, field_idx)\n",
    "#         update_!(opt, field, d_field)\n",
    "#     end\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "augment! (generic function with 2 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment!(x, y) = nothing\n",
    "\n",
    "function augment!(layer::SprAffine, frac::AbstractFloat)\n",
    "    # redraw fraction of the layer's Weights\n",
    "    I, J, V = findnz(layer.W)\n",
    "    #println(length(V))\n",
    "    len_augmented = floor(Int, frac * length(V))\n",
    "    len_preserved = length(V) - len_augmented\n",
    "    #println(len_augmented, \" \", len_preserved)\n",
    "    elems = Set(zip(I, J))\n",
    "    indices = sortperm(abs.(V), rev=true)[1:len_preserved]\n",
    "    I = I[indices]\n",
    "    J = J[indices]\n",
    "    V = V[indices]\n",
    "    I_ = similar(I, len_augmented)\n",
    "    J_ = similar(J, len_augmented)\n",
    "    V_ = similar(V, len_augmented)\n",
    "    index = 1\n",
    "    while index <= len_augmented\n",
    "        i = sample(1:size(layer.W, 1))\n",
    "        j = sample(1:size(layer.W, 2))\n",
    "        if (i,j) in elems\n",
    "            continue\n",
    "        end\n",
    "        push!(elems, (i, j))\n",
    "        I_[index] = i\n",
    "        J_[index] = j\n",
    "        V_[index] = randn()\n",
    "        index += 1\n",
    "    end\n",
    "    append!(I, I_)\n",
    "    append!(J, J_)\n",
    "    #println(length(V))\n",
    "    append!(V, V_)\n",
    "#     for index in 1:length(layer.W.nzval)\n",
    "#         layer.W.nzval[index] = 0.\n",
    "#     end\n",
    "#     droptol!(layer.W, Inf, trim=true)\n",
    "    layer.W = sparse(I, J, V, size(layer.W)...)\n",
    "    #println(length(V))\n",
    "    #println(length(layer.W.nzval))\n",
    "    #println(\" ---\")\n",
    "    return layer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_augmenting! (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_augmenting!(opt, loss, model, (X, y), (X_val, y_val), batch_size, num_epochs,\n",
    "                           augmentation_period, augmentation_fraction)\n",
    "    X_val_sp = sparse(X_val)\n",
    "    for epoch in 1:num_epochs\n",
    "        println(\"Epoch: $epoch\")\n",
    "        perm = shuffle(1:size(X, 2))\n",
    "        X_sp = sparse(X[:, perm])\n",
    "        y_ = y[:, perm]\n",
    "        batch_indices = 1:batch_size:(size(X, 2) - batch_size + 1)\n",
    "        for (index, batch_index) in enumerate(batch_indices)\n",
    "            #print(\"Batch $index/$(length(batch_indices)) \")\n",
    "            loss_local, back = pullback(model) do model\n",
    "                loss(model,\n",
    "                     X_sp[:, 1:batch_size],\n",
    "                     y_[:, 1:batch_size])\n",
    "            end;\n",
    "            #print(round(loss_local, digits=4), \" \")\n",
    "            grads = back(1)[1];\n",
    "            #print(\"g \")\n",
    "            update_!(opt, model, grads)\n",
    "            acc = accuracy(model, X_val_sp, y_val)\n",
    "            print(round(acc, digits=4), \" \")\n",
    "            #print(\"u \")\n",
    "            #println(length(model[1].W.nzval) / length(model[1].W), \" \")\n",
    "            is_very_last_batch = (epoch == num_epochs) && (batch_index > size(X, 2) - 2batch_size)\n",
    "            if (index % augmentation_period == 0) && !is_very_last_batch\n",
    "                for layer in model.layers\n",
    "                    augment!(layer, augmentation_fraction)\n",
    "                end\n",
    "                print(\"\\n\")\n",
    "                #println(\"Augmented $augmentation_fraction of weights.\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "0.1004 0.1088 0.1181 0.1255 0.1313 0.138 \n",
      "0.1403 0.1461 0.153 0.1597 0.1664 0.1722 \n",
      "0.1687 0.1731 0.1772 0.1807 0.1852 0.1898 \n",
      "0.1732 0.1793 0.1849 0.1897 0.194 0.2014 \n",
      "0.1707 0.1758 0.1851 0.1932 0.1973 0.2039 \n",
      "0.2186 0.2231 0.2284 0.2307 0.2348 0.2361 \n",
      "0.1933 0.206 0.2135 0.2192 0.2243 0.2292 \n",
      "0.2372 0.2485 0.2538 0.2573 0.2608 0.2643 \n",
      "0.2488 0.2522 0.2544 0.2559 0.2569 0.2593 \n",
      "0.2359 0.2499 0.2577 0.2647 0.269 0.2714 \n",
      "0.2832 0.2868 0.2888 0.2897 0.2917 0.2925 \n",
      "0.2813 0.2862 0.2862 0.2858 0.2872 0.2876 \n",
      "0.2887 0.2999 0.3052 0.3098 0.313 0.3142 \n",
      "0.2486 0.2521 0.2552 0.2584 0.2607 0.2638 \n",
      "0.2288 0.2368 0.2407 0.2471 0.2517 0.2536 \n",
      "0.2612 0.2669 0.2699 0.2721 0.2751 0.277 \n",
      "0.2698 0.2706 0.2745 0.2786 0.2797 0.2821 \n",
      "0.2885 0.2933 0.2988 0.3029 0.3052 0.3083 \n",
      "0.2921 0.3018 0.3065 0.3117 0.3164 0.3181 \n",
      "0.2741 0.279 0.2832 0.2875 0.2921 0.2955 \n",
      "0.2818 0.2848 0.2903 0.2938 0.2981 0.2997 \n",
      "0.2893 0.2982 0.305 0.3094 0.3132 0.3168 \n",
      "0.3303 0.3324 0.3352 0.3358 0.337 0.3402 \n",
      "0.3137 0.3209 0.3249 0.3277 0.3311 0.333 \n",
      "0.3289 0.3358 0.3403 0.3426 0.3435 0.3449 \n",
      "0.3658 0.3668 0.3678 0.3672 0.3676 0.3675 \n",
      "0.324 0.3303 0.3379 0.344 0.3498 0.3548 \n",
      "0.3612 0.367 0.3708 0.3732 0.3743 0.3739 \n",
      "0.3388 0.3452 0.352 0.3568 0.359 0.3618 \n",
      "0.3531 0.3558 0.3589 0.3621 0.3655 0.3666 \n",
      "0.3659 0.3693 0.3721 0.3744 0.3757 0.377 \n",
      "0.3367 0.3445 0.3472 0.3506 0.352 0.3528 \n",
      "0.3342 0.34 0.3417 0.3427 0.3443 0.3448 \n",
      "0.3723 0.377 0.3808 0.3856 0.3886 0.3908 \n",
      "0.4092 0.4112 0.4118 0.414 0.4148 0.4149 \n",
      "0.3731 0.381 0.3873 0.3921 0.3938 0.3966 \n",
      "0.3738 0.3791 0.3834 0.3862 0.3881 0.3903 \n",
      "0.4152 0.4175 0.4203 0.4216 0.4226 0.4237 \n",
      "0.419 0.4212 0.4238 0.4259 0.4266 0.4278 \n",
      "0.421 0.4249 0.4282 0.4294 0.43 0.4299 \n",
      "0.4052 0.4092 0.4112 0.4115 0.4126 0.4135 \n",
      "0.4171 0.4191 0.4204 0.4216 0.4218 0.4214 \n",
      "0.3683 0.372 0.3721 0.3732 0.3747 0.3747 \n",
      "0.3621 0.3638 0.3664 0.3673 0.3685 0.369 \n",
      "0.3733 0.3757 0.3784 0.3802 0.381 0.381 \n",
      "0.3296 0.3343 0.338 0.3427 0.3467 0.3499 \n",
      "0.3584 0.3608 0.3645 0.3652 0.3659 0.3666 \n",
      "0.3673 0.3697 0.3728 0.3752 0.3764 0.377 \n",
      "0.3409 0.3457 0.3492 0.3507 0.3518 0.3519 \n",
      "0.3189 0.3258 0.3296 0.3338 0.335 0.3362 \n",
      "0.3519 0.3522 0.351 0.3518 0.3515 0.3517 \n",
      "0.3611 0.3634 0.3653 0.3668 0.3671 0.3674 \n",
      "0.344 0.3436 0.3434 0.3428 0.343 0.3432 \n",
      "0.3312 0.3352 0.3384 0.3413 0.3446 0.3453 \n",
      "0.3652 0.3701 0.374 0.3767 0.379 0.3792 \n",
      "0.3958 0.3966 0.3972 0.3994 0.4009 0.4017 \n",
      "0.3656 0.3767 0.3798 0.3814 0.3818 0.3826 \n",
      "0.3805 0.3848 0.3856 0.3873 0.3892 0.3907 \n",
      "0.4069 0.4085 0.4089 0.4075 0.4081 0.4073 \n",
      "0.41 0.4102 0.4084 0.4073 0.4065 0.4058 \n",
      "0.4051 0.4038 0.4024 0.4031 0.4027 0.4018 \n",
      "0.3974 0.4065 0.4127 0.4158 0.419 0.4207 \n",
      "0.4138 0.4167 0.4191 0.4208 0.4209 0.4218 \n",
      "0.4299 0.4286 0.4282 0.4268 0.427 0.4277 \n",
      "0.3924 0.3985 0.4024 0.4038 0.4055 0.4063 \n",
      "0.425 0.425 0.4251 0.4235 0.4237 0.4241 \n",
      "0.4239 0.4258 0.4272 0.428 0.428 0.4276 \n",
      "0.4225 0.4236 0.4241 0.4246 0.4251 0.4253 \n",
      "0.3773 0.3882 0.3956 0.4024 0.4057 0.408 \n",
      "0.4261 0.4288 0.4302 0.4304 0.4324 0.432 \n",
      "0.4361 0.4382 0.4392 0.4409 0.4418 0.4419 \n",
      "0.4339 0.4362 0.4402 0.4429 0.446 0.4471 \n",
      "0.4413 0.4425 0.4429 0.4429 0.4439 0.4445 \n",
      "0.4384 0.4402 0.4417 0.4427 0.4431 0.4433 \n",
      "0.4127 0.4145 0.4148 0.4154 0.4168 0.4183 \n",
      "0.4148 0.4175 0.42 0.4212 0.4234 0.426 \n",
      "0.4204 0.421 0.4208 0.4211 0.422 0.4219 \n",
      "0.4125 0.4162 0.4179 0.4193 0.4191 0.4206 \n",
      "0.4162 0.4171 0.4207 0.4208 0.4218 0.4221 \n",
      "0.4043 0.407 0.4102 0.4118 0.4126 0.4129 \n",
      "0.3795 0.3827 0.3822 0.3847 0.3863 0.3888 \n",
      "0.387 0.3954 0.4024 0.4071 0.4118 0.414 \n",
      "0.422 0.4235 0.4244 0.4251 0.425 0.4256 \n",
      "0.4264 0.4271 0.4256 0.4263 0.4269 0.4268 \n",
      "0.4524 0.4522 0.4524 0.4532 0.4527 0.4521 \n",
      "0.4236 0.4245 0.4254 0.425 0.4251 0.4253 \n",
      "0.4441 0.445 0.4457 0.4454 0.4462 0.4468 \n",
      "0.4258 0.427 0.4265 0.4278 0.4277 0.4278 \n",
      "0.3851 0.3909 0.3974 0.4042 0.4103 0.4153 \n",
      "0.4258 0.4253 0.4262 0.4271 0.4288 0.4299 \n",
      "0.4107 0.4148 0.4206 0.4244 0.4245 0.425 \n",
      "0.4432 0.4446 0.4459 0.4466 0.4472 0.4476 \n",
      "0.4268 0.43 0.4318 0.4337 0.4337 0.434 \n",
      "0.4443 0.4447 0.445 0.4454 0.4456 0.4457 \n",
      "0.4684 0.4685 0.4684 0.4677 0.4673 0.4673 \n",
      "0.4473 0.447 0.4476 0.4484 0.4485 0.4487 \n",
      "0.4268 0.4265 0.4275 0.4285 0.4282 0.429 \n",
      "0.4024 0.4131 0.4179 0.4228 0.4234 0.4252 \n",
      "0.4312 0.4348 0.4363 0.4378 0.4375 0.4382 \n",
      "0.4457 0.4468 0.4468 0.4469 0.4472 0.4472 \n",
      "0.4261 0.4272 0.4278 0.4283 0.4289 0.4295 \n",
      "0.4129 0.4148 0.4175 0.4182 0.4186 0.4192 \n",
      "0.3819 0.3868 0.3912 0.3942 0.396 0.3977 \n",
      "0.3955 0.3982 0.3992 0.4 0.3998 0.3999 \n",
      "0.3994 0.4013 0.405 0.4057 0.4072 0.4073 \n",
      "0.4301 0.4306 0.4306 0.4312 0.4315 0.4321 \n",
      "0.4488 0.4507 0.4536 0.4558 0.4571 0.4578 \n",
      "0.4556 0.4562 0.4564 0.4567 0.4567 0.4568 \n",
      "0.4194 0.421 0.4218 0.4218 0.4219 0.4216 \n",
      "0.4232 0.4261 0.4272 0.4279 0.4291 0.4302 \n",
      "0.4458 0.4467 0.4487 0.4508 0.4524 0.4518 \n",
      "0.4389 0.4422 0.4455 0.4457 0.446 0.4459 \n",
      "0.3898 0.393 0.3951 0.3967 0.399 0.3996 \n",
      "0.423 0.4238 0.4242 0.4248 0.4251 0.4257 \n",
      "0.4148 0.4171 0.4191 0.4184 0.4188 0.4198 \n",
      "0.4136 0.4145 0.4176 0.4208 0.4226 0.4239 \n",
      "0.4138 0.4165 0.4198 0.4215 0.4219 0.4222 \n",
      "0.4071 0.4102 0.4126 0.4143 0.4144 0.4145 \n",
      "0.429 0.4293 0.4297 0.4302 0.43 0.4301 \n",
      "0.3968 0.3998 0.4008 0.4016 0.4029 0.405 \n",
      "0.4108 0.4109 0.411 0.4114 0.4114 0.4114 \n",
      "0.4253 0.425 0.4258 0.4257 0.426 0.4258 \n",
      "0.4005 0.4025 0.4037 0.4053 0.4062 0.407 \n",
      "0.4072 0.4098 0.4128 0.4135 0.4161 0.4168 \n",
      "0.4226 0.4242 0.4241 0.4233 0.4228 0.4226 \n",
      "0.4168 0.4181 0.4182 0.4182 0.4181 0.4181 \n",
      "0.4014 0.4012 0.4015 0.4019 0.4019 0.4019 \n",
      "0.4125 0.4135 0.414 0.4139 0.414 0.4139 \n",
      "0.4076 0.4126 0.4161 0.4161 0.4161 0.4158 \n",
      "0.3879 0.3893 0.3919 0.3923 0.3926 0.3928 \n",
      "0.3928 0.3939 0.3948 0.3948 0.395 0.3951 \n",
      "0.4088 0.41 0.4108 0.4111 0.4118 0.4119 \n",
      "0.4138 0.4133 0.4134 0.4137 0.4136 0.4136 \n",
      "0.4012 0.4017 0.4018 0.4017 0.4018 0.402 \n",
      "0.4208 0.4199 0.4193 0.4192 0.418 0.4175 \n",
      "0.4022 0.4015 0.4019 0.4017 0.4024 0.4032 \n",
      "0.4136 0.4138 0.4142 0.4138 0.4142 0.4142 \n",
      "0.3991 0.4026 0.4036 0.4029 0.4026 0.4022 \n",
      "0.3577 0.3651 0.3715 0.3758 0.379 0.3806 \n",
      "0.385 0.3853 0.3861 0.3858 0.3855 0.3854 \n",
      "0.4032 0.4032 0.4033 0.4038 0.4043 0.4048 \n",
      "0.4242 0.4246 0.4253 0.4262 0.4252 0.4258 \n",
      "0.4303 0.4362 0.4413 0.4426 0.4452 0.4454 \n",
      "0.4498 0.4492 0.4492 0.4492 0.4488 0.4492 \n",
      "0.4412 0.4423 0.4422 0.4427 0.4428 0.4429 \n",
      "0.434 0.4333 0.4332 0.4331 0.4332 0.4331 \n",
      "0.4242 0.4244 0.4242 0.4246 0.4248 0.4247 \n",
      "0.4123 0.413 0.4128 0.4127 0.4128 0.4128 \n",
      "0.4278 0.4282 0.4282 0.4288 0.4288 0.4294 \n",
      "0.4311 0.4323 0.4334 0.4345 0.4345 0.4348 \n",
      "0.4286 0.4286 0.4288 0.4288 0.4288 "
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] (::Zygote.var\"#995#997\"{SparseMatrixCSC{Float32,Int64},Tuple{Colon,UnitRange{Int64}}})(::SparseMatrixCSC{Float32,Int64}) at /home/olszewskip/.julia/packages/Zygote/oMScO/src/lib/array.jl:45",
      " [2] #2634#back at /home/olszewskip/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]",
      " [3] #3 at ./In[10]:13 [inlined]",
      " [4] (::Zygote.var\"#28#29\"{typeof(∂(λ))})(::Int64) at /home/olszewskip/.julia/packages/Zygote/oMScO/src/compiler/interface.jl:38",
      " [5] train_augmenting!(::Flux.Optimise.Descent, ::typeof(loss), ::Chain{Tuple{SprAffine{SparseMatrixCSC{Float32,Int64},Array{Float32,1},typeof(identity)},SprAffine{SparseMatrixCSC{Float32,Int64},Array{Float32,1},typeof(identity)}}}, ::Tuple{Array{Float32,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}, ::Tuple{Array{Float32,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}, ::Int64, ::Int64, ::Int64, ::Float64) at ./In[10]:18",
      " [6] top-level scope at In[41]:2"
     ]
    }
   ],
   "source": [
    "model = get_model(size(X, 1), 100, size(y, 1), 0.1)\n",
    "train_augmenting!(Optimise.Descent(0.1), loss, model, (X, y), (X_val, y_val),\n",
    "                  64, 1, 6, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "0.1572 0.1625 0.1689 0.1762 0.18 0.1831 0.1843 0.1878 0.1905 0.1938 0.1971 0.2008 0.2052 0.2088 0.2135 0.2158 \n",
      "0.1507 0.1572 0.164 0.1698 0.1743 0.1794 0.1835 0.1883 0.1925 0.1964 0.2006 0.2042 0.2078 0.2106 0.2133 0.2178 \n",
      "0.2382 0.2448 0.2452 0.2502 0.2525 0.2568 0.2615 0.2672 0.2737 0.2792 0.2855 0.292 0.2966 0.3006 0.3056 0.3098 \n",
      "0.2362 0.2418 0.2439 0.2507 0.2576 0.2638 0.2713 0.2782 0.2844 0.2917 0.2967 0.301 0.3053 0.3098 0.3136 0.3159 \n",
      "0.2099 0.221 0.2312 0.2418 0.2512 0.2583 0.2638 0.2696 0.2751 0.2777 0.281 0.2852 0.2889 0.2902 0.2921 0.2949 \n",
      "0.2578 0.2673 0.2733 0.2805 0.2864 0.2945 0.3026 0.3104 0.3167 0.323 0.3282 0.3312 0.3362 0.3398 0.344 0.3452 \n",
      "0.2419 0.2656 0.2853 0.2977 0.3051 0.3093 0.3132 0.3177 0.3219 0.3268 0.3312 0.3356 0.3392 0.3442 0.3481 0.3509 \n",
      "0.2558 0.2774 0.2995 0.3192 0.3264 0.3355 0.3398 0.3445 0.3485 0.3502 0.3529 0.3548 0.3562 0.3581 0.3597 0.3621 \n",
      "0.1605 0.1744 0.1862 0.1984 0.2089 0.2185 0.2252 0.2314 0.2358 0.2408 0.2456 0.2498 0.2536 0.2568 0.2602 0.2627 \n",
      "0.2448 0.2586 0.2679 0.2778 0.2852 0.2916 0.2964 0.3024 0.3071 0.3112 0.3163 0.3202 0.3248 0.3277 0.329 0.3308 \n",
      "0.3114 0.327 0.3397 0.3494 0.358 0.3644 0.3694 0.3723 0.3748 0.3781 0.3798 0.3824 0.3834 0.3852 0.3856 0.3875 \n",
      "0.2908 0.3014 0.3092 0.3174 0.3242 0.3307 0.3353 0.341 0.3448 0.3476 0.3508 0.3545 0.358 0.3612 0.3643 0.3677 \n",
      "0.3102 0.3247 0.3348 0.3429 0.3506 0.3568 0.3627 0.365 0.3678 0.3697 0.3716 0.3742 0.3756 0.3769 0.3782 0.3787 \n",
      "0.3599 0.3653 0.3711 0.3775 0.3829 0.3903 0.3946 0.3984 0.4027 0.4061 0.4075 0.4082 0.4096 0.4117 0.4128 0.4137 \n",
      "0.3122 0.3177 0.3232 0.3265 0.329 0.3316 0.3342 0.3373 0.3399 0.3422 0.3448 0.3472 0.3492 0.3509 0.3521 0.3524 \n",
      "0.2803 0.2927 0.3052 0.3141 0.322 0.3285 0.3318 0.335 0.3377 0.3402 0.343 0.3462 0.3492 0.3512 0.3531 0.3546 \n",
      "0.3412 0.3455 0.3492 0.3532 0.3537 0.355 0.3571 0.3577 0.3599 0.3607 0.3612 0.3631 0.3636 0.3648 0.3652 0.3658 \n",
      "0.2966 0.3037 0.3094 0.3151 0.3198 0.3262 0.3302 0.3348 0.3373 0.3398 0.3424 0.3447 0.3469 0.3481 0.3495 0.3507 \n",
      "0.2808 0.2912 0.301 0.3094 0.3171 0.3223 0.3262 0.3317 0.3345 0.3348 0.3365 0.3372 0.3375 0.3396 0.3405 0.3417 \n",
      "0.3804 0.3884 0.3952 0.3992 0.4024 0.4035 0.4033 0.4033 0.404 0.4052 0.4068 0.4073 0.4085 0.4093 0.4095 0.4108 \n",
      "0.2935 0.3048 0.3139 0.3187 0.3216 0.3246 0.3265 0.3291 0.332 0.3342 0.3348 0.336 0.3377 0.3392 0.3408 0.3426 \n",
      "0.2736 0.2855 0.2938 0.303 0.3095 0.3132 0.3162 0.3182 0.3198 0.3227 0.3239 0.3252 0.3269 0.328 0.33 0.3318 \n",
      "0.3383 0.3436 0.3511 0.3526 0.3529 0.3535 0.3538 0.3554 0.3579 0.3592 0.3602 0.361 0.3618 0.3629 0.3642 0.3653 \n",
      "0.3204 0.3219 0.3245 0.3254 0.3268 0.3282 0.3288 0.329 0.3298 0.3302 0.3311 0.3322 0.3338 0.3346 0.337 0.3378 \n",
      "0.2852 0.3093 0.3226 0.3292 0.3352 0.3398 0.3422 0.3439 0.346 0.3477 0.3496 0.3514 0.3522 0.3534 0.3538 0.3546 \n",
      "0.3619 0.3659 0.3701 0.3727 0.3744 0.377 0.3787 0.3802 0.383 0.3844 0.3862 0.3872 0.3888 0.3903 0.3918 0.3909 \n",
      "0.3597 0.3656 0.3709 0.374 0.3772 0.3792 0.3802 0.3809 0.3812 0.3822 0.3839 0.3854 0.3861 0.3873 0.3882 0.3886 \n",
      "0.3831 0.3943 0.4013 0.4078 0.4127 0.4173 0.4202 0.4249 0.4274 0.4287 0.4309 0.4333 0.4352 0.4357 0.4365 0.4365 \n",
      "0.3423 0.3564 0.3666 0.3747 0.3822 0.3862 0.391 0.3948 0.3978 0.3999 0.4025 0.4035 0.4052 0.4063 0.4068 0.4073 \n",
      "0.4191 0.4304 0.4409 0.4436 0.4468 0.4489 0.4512 0.4512 0.4531 0.4539 0.4547 0.4546 0.4546 0.4552 0.4553 0.4557 \n",
      "0.4153 0.4198 0.421 0.4228 0.4262 0.4284 0.4298 0.4314 0.4318 0.4334 0.4348 0.4352 0.4382 0.4388 0.439 0.4388 \n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] (::Zygote.var\"#995#997\"{SparseMatrixCSC{Float32,Int64},Tuple{Colon,UnitRange{Int64}}})(::SparseMatrixCSC{Float32,Int64}) at /home/olszewskip/.julia/packages/Zygote/oMScO/src/lib/array.jl:45",
      " [2] #2634#back at /home/olszewskip/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49 [inlined]",
      " [3] #41 at ./In[172]:13 [inlined]",
      " [4] (::Zygote.var\"#28#29\"{typeof(∂(λ))})(::Int64) at /home/olszewskip/.julia/packages/Zygote/oMScO/src/compiler/interface.jl:38",
      " [5] train_augmenting!(::Flux.Optimise.Descent, ::typeof(loss), ::Chain{Tuple{SprAffine{SparseMatrixCSC{Float32,Int64},Array{Float32,1},typeof(identity)},SprAffine{SparseMatrixCSC{Float32,Int64},Array{Float32,1},typeof(identity)}}}, ::Tuple{Array{Float32,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}, ::Tuple{Array{Float32,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}, ::Int64, ::Int64, ::Int64, ::Float64) at ./In[172]:18",
      " [6] top-level scope at In[176]:2"
     ]
    }
   ],
   "source": [
    "model = get_model(size(X, 1), 100, size(y, 1), 0.1)\n",
    "train_augmenting!(Optimise.Descent(0.1), loss, model, (X, y), (X_val, y_val),\n",
    "                  64, 1, 16, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmenting!(Optimise.Descent(0.2), loss, model, (X, y), (X_val, y_val),\n",
    "                  64, 1, 5, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model, sparse(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×60000 Array{Float32,2}:\n",
       " -15.2978    39.3126     0.471826  …   11.5914     27.8094    16.3412  \n",
       " -14.4236   -31.6884   -19.7033       -24.9329    -18.037    -15.8129  \n",
       "   1.94016  -64.7316     8.30274      -31.5712    -10.1424   -36.3982  \n",
       "  21.5777    21.9106    12.6699       -16.7958      9.84951    0.533863\n",
       " -49.4807   -50.8089   -13.4105       -28.6316    -32.741    -12.1242  \n",
       "  11.3429     1.226     -2.5209    …    9.23097    29.3412    -6.88847 \n",
       " -12.4105   -22.5244    20.2417        -0.273212    5.67532    7.27039 \n",
       "  -2.42519   28.6859    -4.85765       -6.10574     7.92254   29.8143  \n",
       "   3.65144   -6.33468    8.74955       15.3897    -11.0637    -2.80236 \n",
       " -14.0099     7.8312     0.470142      15.9757     11.3657    12.9205  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array(model(sparse(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       "  6\n",
       "  1\n",
       "  5\n",
       "  2\n",
       " 10\n",
       "  3\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  5"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onecold(y[:, 1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/468 Loss: 12.903942\n",
      "Batch 2/468 Loss: 7.998606\n",
      "Batch 3/468 Loss: 5.978869\n",
      "Batch 4/468 Loss: 4.7153387\n",
      "Batch 5/468 Loss: 3.8186448\n",
      "Batch 6/468 Loss: 3.1671693\n",
      "Batch 7/468 Loss: 2.6950784\n",
      "Batch 8/468 Loss: 2.348345\n",
      "Batch 9/468 Loss: 2.077857\n",
      "Batch 10/468 Loss: 1.8604236\n",
      "Augmented 0.3 of weights.\n",
      "Batch 11/468 Loss: 6.780545\n",
      "Batch 12/468 Loss: 4.6947923\n",
      "Batch 13/468 Loss: 3.576496\n",
      "Batch 14/468 Loss: 3.0275183\n",
      "Batch 15/468 Loss: 2.3009427\n",
      "Batch 16/468 Loss: 1.9193074\n",
      "Batch 17/468 Loss: 1.6593554\n",
      "Batch 18/468 Loss: 1.4660087\n",
      "Batch 19/468 Loss: 1.3267179\n",
      "Batch 20/468 Loss: 1.2275671\n",
      "Augmented 0.3 of weights.\n",
      "Batch 21/468 Loss: 6.5453057\n",
      "Batch 22/468 Loss: 4.088305\n",
      "Batch 23/468 Loss: 2.9599335\n",
      "Batch 24/468 Loss: 1.8741505\n",
      "Batch 25/468 Loss: 1.1792691\n",
      "Batch 26/468 Loss: 0.8042897\n",
      "Batch 27/468 Loss: 0.651187\n",
      "Batch 28/468 Loss: 0.50910723\n",
      "Batch 29/468 Loss: 0.408691\n",
      "Batch 30/468 Loss: 0.34578425\n",
      "Augmented 0.3 of weights.\n",
      "Batch 31/468 Loss: 8.25316\n",
      "Batch 32/468 Loss: 3.3610651\n",
      "Batch 33/468 Loss: 1.8641839\n",
      "Batch 34/468 Loss: 1.6502843\n",
      "Batch 35/468 Loss: 1.7615561\n",
      "Batch 36/468 Loss: 0.5374702\n",
      "Batch 37/468 Loss: 0.30581087\n",
      "Batch 38/468 Loss: 0.19012406\n",
      "Batch 39/468 Loss: 0.11263185\n",
      "Batch 40/468 Loss: 0.08237183\n",
      "Augmented 0.3 of weights.\n",
      "Batch 41/468 Loss: 10.097649\n",
      "Batch 42/468 Loss: NaN\n",
      "Batch 43/468 Loss: NaN\n",
      "Batch 44/468 Loss: NaN\n",
      "Batch 45/468 Loss: NaN\n",
      "Batch 46/468 Loss: NaN\n",
      "Batch 47/468 Loss: NaN\n",
      "Batch 48/468 Loss: NaN\n",
      "Batch 49/468 Loss: NaN\n",
      "Batch 50/468 Loss: NaN\n",
      "Augmented 0.3 of weights.\n",
      "Batch 51/468 Loss: NaN\n",
      "Batch 52/468 Loss: NaN\n",
      "Batch 53/468 Loss: NaN\n",
      "Batch 54/468 Loss: NaN\n",
      "Batch 55/468 Loss: NaN\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] (::Zygote.var\"#995#997\"{SparseMatrixCSC{Float32,Int64},Tuple{Colon,UnitRange{Int64}}})(::SparseMatrixCSC{Float32,Int64}) at /home/olszewskip/.julia/packages/Zygote/oMScO/src/lib/array.jl:45",
      " [2] (::Zygote.var\"#2634#back#991\"{Zygote.var\"#995#997\"{SparseMatrixCSC{Float32,Int64},Tuple{Colon,UnitRange{Int64}}}})(::SparseMatrixCSC{Float32,Int64}) at /home/olszewskip/.julia/packages/ZygoteRules/6nssF/src/adjoint.jl:49",
      " [3] #25 at ./In[43]:11 [inlined]",
      " [4] (::typeof(∂(λ)))(::Int64) at /home/olszewskip/.julia/packages/Zygote/oMScO/src/compiler/interface2.jl:0",
      " [5] #28 at /home/olszewskip/.julia/packages/Zygote/oMScO/src/compiler/interface.jl:38 [inlined]",
      " [6] train_augmenting!(::Flux.Optimise.Descent, ::typeof(loss), ::Chain{Tuple{SprAffine{SparseMatrixCSC{Float32,Int64},Array{Float32,1},typeof(identity)},SprAffine{SparseMatrixCSC{Float32,Int64},Array{Float32,1},typeof(identity)},typeof(softmax)}}, ::Tuple{Array{Float32,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}, ::Int64, ::Int64, ::Int64, ::Float64) at ./In[43]:16",
      " [7] top-level scope at In[44]:2"
     ]
    }
   ],
   "source": [
    "# model = get_model()\n",
    "# train_augmenting!(Optimise.Descent(0.3), loss, model, (X, y), 128, 10, 10, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
